{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba0d16b-bca9-4da1-ab23-f76ad4799ad8",
   "metadata": {},
   "source": [
    "# **Level 3: The Archives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ab30f-3a7e-4374-961d-c5210ad1afcd",
   "metadata": {},
   "source": [
    "## **Part 4: Vector Embeddings â€“ The Language of Meaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146977f6-6476-42db-a773-45c9f00defcc",
   "metadata": {},
   "source": [
    "\n",
    "Hello everyone, and welcome back\\! In our last session, we accomplished a crucial step in building our RAG system: we took large, unstructured documents and, using **Text Splitters**, we chunked them down into manageable pieces. This was a critical preprocessing step to ensure our documents can fit into the context window of a Large Language Model.\n",
    "\n",
    "But now, we're faced with a new, and perhaps more interesting, problem.\n",
    "\n",
    "-----\n",
    "\n",
    "## **1. Recap & Bridge: The Search Problem**\n",
    "\n",
    "So, we have a list of `Document` chunks. Fantastic. Now what?\n",
    "\n",
    "Imagine a user asks our system a question: *\"How do I pay my bill?\"*\n",
    "\n",
    "We might have a chunk of text from our knowledge base that says: *\"Procedures for remitting payment are outlined in Section 3 of the customer agreement.\"*\n",
    "\n",
    "Notice the problem? The user's question and the relevant document chunk don't share any significant keywords. \"Pay\" is not in our chunk. \"Bill\" is not in our chunk. A traditional keyword search (like `Ctrl+F`) would completely fail here. It would look for the exact words \"pay\" and \"bill\" and come up empty-handed, even though the *meaning* of the user's question is directly addressed in that chunk.\n",
    "\n",
    "This is the fundamental limitation of keyword-based search. It's literal, rigid, and lacks an understanding of language's nuance, synonyms, and paraphrasing.\n",
    "\n",
    "To build a truly intelligent RAG system, we need a way to search not by *words*, but by **meaning**. We need a way to determine that \"paying a bill\" is semantically very similar to \"remitting payment.\"\n",
    "\n",
    "This is where **Vector Embeddings** come in. They are the solution to this search problem and the core component that enables **semantic search**.\n",
    "\n",
    "-----\n",
    "\n",
    "## **2. What are Embeddings? (Making Text Understandable to Computers)**\n",
    "\n",
    "This might sound complex, but the core idea is surprisingly intuitive.\n",
    "\n",
    "> **Simple Definition:** Vector embeddings are numerical representations of data, like text, that capture its underlying meaning and context. We turn words and sentences into a long list of numbers, called a **vector**.\n",
    "\n",
    "Computers don't understand words; they understand numbers. Embeddings are the bridge that translates the rich, semantic meaning of human language into the mathematical world of a computer.\n",
    "\n",
    "Let's use a couple of analogies to make this crystal clear.\n",
    "\n",
    "**Analogy 1: Location on a Map ðŸ—ºï¸**\n",
    "\n",
    "Imagine you have a giant map. You can represent the location of any city using two numbers: its latitude and its longitude. For example, Paris might be (48.85, 2.35) and London might be (51.50, -0.12).\n",
    "\n",
    "Cities that are geographically close, like Paris and Brussels, will have coordinates that are numerically close to each other. Cities that are far apart, like Paris and Tokyo, will have very different coordinates.\n",
    "\n",
    "Vector embeddings do the exact same thing, but for **meaning**. Instead of a 2-dimensional map, we use a much higher-dimensional \"semantic space\" (often with hundreds or even thousands of dimensions, like 768 or 1536). In this space:\n",
    "\n",
    "  * The sentence *\"The feline rested on the rug.\"* will be \"plotted\" very close to...\n",
    "  * The sentence *\"The cat sat on the mat.\"*\n",
    "  * But it will be plotted very far away from *\"The car drove on the highway.\"*\n",
    "\n",
    "**Analogy 2: Describing with Attributes ðŸŽ**\n",
    "\n",
    "Think about how you might describe a piece of fruit using only numbers on a scale of -1 to 1. You could create a vector of attributes:\n",
    "`[is_round, is_sweet, is_crunchy, color_is_red]`\n",
    "\n",
    "  * A **Granny Smith Apple** might be: `[0.9, -0.8, 0.9, -0.7]` (very round, not sweet, very crunchy, not red)\n",
    "  * A **Red Delicious Apple** might be: `[0.8, 0.7, 0.8, 0.9]` (very round, sweet, very crunchy, very red)\n",
    "  * An **Orange** might be: `[0.9, 0.6, -0.5, -0.5]` (very round, sweet, not crunchy, not red)\n",
    "\n",
    "By comparing these numbers, a computer can see that the two apples are more similar to each other than they are to the orange. Embeddings work similarly, but the attributes are abstract linguistic featuresâ€”like tone, subject matter, and contextâ€”that are learned automatically by an AI model.\n",
    "\n",
    "> **Key Idea: Semantic Similarity**\n",
    ">\n",
    "> This is the entire point. If two pieces of text mean similar things, their vector embeddings will be \"close\" to each other in this high-dimensional space. We can measure this \"closeness\" mathematically, which is something computers are exceptionally good at.\n",
    "\n",
    "So, how are these magical number lists created? They are generated by specialized AI models, aptly called **embedding models**. These models have been trained on vast amounts of text from the internet, learning the subtle relationships between words and the contexts in which they appear.\n",
    "\n",
    "-----\n",
    "\n",
    "## **3. Why Do We Need Embeddings for RAG? (The \"Magic\" Behind Semantic Search)**\n",
    "\n",
    "Understanding what embeddings *are* is the first step. Understanding why they are indispensable for RAG is next.\n",
    "\n",
    "  * **Beyond Keywords:** As we saw in our \"pay my bill\" example, embeddings free us from the prison of exact keyword matching. They allow our RAG system to find relevant information even when the user's phrasing is completely different from the source documents. This is the leap from a simple search engine to a true knowledge retrieval system.\n",
    "\n",
    "  * **Contextual Understanding:** Embeddings are not just about single words; they capture the context of the entire sentence or chunk. The word \"apple\" in the sentence \"I ate an apple\" will have a very different embedding vector from the word \"Apple\" in \"I bought a new Apple laptop.\" The embedding model uses the surrounding words to figure out the correct meaning.\n",
    "\n",
    "  * **Efficiency (A Glimpse of the Future):** Once all our document chunks are converted into vectors (lists of numbers), we can store them in a specialized database designed for lightning-fast mathematical searches. These are called **Vector Stores**, and we will dedicate our entire next session to them. For now, just know that searching through millions of vectors is computationally much faster than re-reading and analyzing millions of text chunks for every single query.\n",
    "\n",
    "  * **Clarifying the LLM's Role:** This is a crucial point to remember. The **search and retrieval** part of RAG is powered by comparing the *embedding* of the user's query against the *embeddings* of our document chunks. However, once we find the most relevant chunks, we don't send the numbers to the LLM. We send the **original text of those chunks** to the LLM to provide the context it needs to generate a final answer.\n",
    "\n",
    "-----\n",
    "\n",
    "## **4. Introducing Embedding Models in LangChain**\n",
    "\n",
    "LangChain provides a wonderfully simple and standardized interface for working with dozens of different embedding models. This means you can learn the interface once and then easily swap out models to see which works best for your project.\n",
    "\n",
    "The core of this is the `Embeddings` base class. Any model you use will have two primary, essential methods:\n",
    "\n",
    "1.  `embed_query(text: str) -> List[float]`: This method takes a single string of text (like a user's question) and returns a single vector embedding (a list of floating-point numbers).\n",
    "\n",
    "2.  `embed_documents(texts: List[str]) -> List[List[float]]`: This method takes a list of strings (like all of our document chunks) and returns a list of vectors, one for each input document. This is highly optimized for processing many documents at once.\n",
    "\n",
    "Let's see what this looks like conceptually with a familiar provider, OpenAI.\n",
    "\n",
    "```python\n",
    "# Note: You'll need to have your OPENAI_API_KEY set as an environment variable\n",
    "# pip install langchain-openai\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "# By default, this uses the \"text-embedding-3-small\" model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Let's embed a user query\n",
    "query_text = \"How do I pay my bill?\"\n",
    "query_embedding = embeddings_model.embed_query(query_text)\n",
    "\n",
    "print(f\"Our query text: '{query_text}'\")\n",
    "print(f\"Type of the embedding: {type(query_embedding)}\")\n",
    "print(f\"Length of the embedding vector: {len(query_embedding)}\")\n",
    "print(f\"First 5 elements of the vector: {query_embedding[:5]}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Now, let's embed a list of document chunks\n",
    "document_chunks = [\n",
    "    \"Procedures for remitting payment are outlined in Section 3.\",\n",
    "    \"Our company was founded in 1995.\",\n",
    "    \"To reset your password, please click the 'Forgot Password' link.\"\n",
    "]\n",
    "\n",
    "document_embeddings = embeddings_model.embed_documents(document_chunks)\n",
    "\n",
    "print(f\"We have {len(document_embeddings)} document embeddings.\")\n",
    "print(f\"The embedding for the first document has a length of {len(document_embeddings[0])}.\")\n",
    "\n",
    "```\n",
    "\n",
    "When you run this, you'll see that the `embed_query` method returns a single list of numbers. For OpenAI's `text-embedding-3-small` model, that list will have **1536** numbers\\! That's the dimensionality of its \"semantic map.\" The `embed_documents` method returns a list containing three of these 1536-dimension vectors.\n",
    "\n",
    "This is it. This is the core interaction. You take text in, and you get numbers out.\n",
    "\n",
    "-----\n",
    "\n",
    "## **5. Exploring Different Embedding Models: Free and Open-Source Options**\n",
    "\n",
    "While OpenAI provides excellent, high-performing models, relying solely on a proprietary, paid service isn't always the best option. You might need to run things locally for privacy, you might be on a budget, or you might find that another model simply performs better for your specific data.\n",
    "\n",
    "LangChain makes it easy to switch. Let's explore some powerful alternatives.\n",
    "\n",
    "### **Option 1: OpenAI Embeddings (The Baseline)**\n",
    "\n",
    "We just saw this in action. It's a great starting point because it's high-quality and easy to use.\n",
    "\n",
    "  * **Pros:** Very strong performance, simple API.\n",
    "  * **Cons:** Closed-source, requires an API key, and costs money based on usage.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "# Recap of the OpenAI code\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "embedding_vector = openai_embeddings.embed_query(\"This is a test sentence.\")\n",
    "# print(f\"OpenAI embedding vector length: {len(embedding_vector)}\")\n",
    "```\n",
    "\n",
    "### **Option 2: Hugging Face Embeddings (The Open-Source Powerhouse)**\n",
    "\n",
    "Hugging Face is the center of the universe for open-source AI models. The `HuggingFaceEmbeddings` class in LangChain is your gateway to thousands of models that you can run for **free**, often on your own computer.\n",
    "\n",
    "To use these, we'll need to install a couple of key libraries:\n",
    "\n",
    "```bash\n",
    "pip install langchain-huggingface sentence-transformers\n",
    "```\n",
    "\n",
    "The `sentence-transformers` library is a fantastic Python framework built specifically for creating high-quality sentence and text embeddings. It's the engine that `HuggingFaceEmbeddings` often uses under the hood.\n",
    "\n",
    "So, how do you choose a model? You can browse the [Hugging Face Model Hub](https://huggingface.co/models) and filter for \"Sentence Similarity\" tasks. A fantastic, lightweight, and very popular general-purpose model is `sentence-transformers/all-MiniLM-L6-v2`.\n",
    "\n",
    "Let's see it in action.\n",
    "\n",
    "```python\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Specify the model name we want to use from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# You can specify device='cpu' or device='cuda' if you have a GPU\n",
    "# If you don't specify, it will try to use a GPU if available.\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "# Initialize the HuggingFaceEmbeddings class\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "# The first time you run this, it will download the model (a few hundred MB)\n",
    "# and cache it for future use.\n",
    "print(\"Hugging Face model loaded.\")\n",
    "\n",
    "# The interface is exactly the same!\n",
    "query_text = \"How do I pay my bill?\"\n",
    "hf_query_embedding = hf_embeddings.embed_query(query_text)\n",
    "\n",
    "print(f\"Length of the Hugging Face embedding vector: {len(hf_query_embedding)}\")\n",
    "print(f\"First 5 elements of the vector: {hf_query_embedding[:5]}\")\n",
    "```\n",
    "\n",
    "**Pro Tip:** Notice that the `all-MiniLM-L6-v2` model produces a vector of length **384**. This is different from OpenAI's 1536. The dimensionality is a characteristic of the specific model architecture. This has trade-offs in terms of performance versus computational cost, which we'll discuss in best practices.\n",
    "\n",
    "  * **Pros:** Free to use, can run locally (great for privacy and offline use), huge variety of models to choose from.\n",
    "  * **Cons:** Requires you to download the model, can consume local CPU/RAM, might not be as performant as the top-tier proprietary models without fine-tuning.\n",
    "\n",
    "### **Option 3: Google Embeddings (Another Major Player)**\n",
    "\n",
    "Google, through its Gemini and other AI platforms, also offers excellent embedding models. LangChain integrates these through the `GoogleGenerativeAIEmbeddings` class.\n",
    "\n",
    "```bash\n",
    "pip install langchain-google-genai\n",
    "```\n",
    "\n",
    "You would use it very similarly, assuming you have a Google AI API key set up.\n",
    "\n",
    "```python\n",
    "# You would need to have your GOOGLE_API_KEY set up\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "# google_vector = google_embeddings.embed_query(\"This is a test sentence from Google.\")\n",
    "# print(f\"Google embedding vector length: {len(google_vector)}\")\n",
    "```\n",
    "\n",
    "The key takeaway here is not to memorize every single provider, but to understand that the **`embed_query` and `embed_documents` methods are the universal standard**. This makes your code modular and adaptable.\n",
    "\n",
    "-----\n",
    "\n",
    "## **6. Comparing Embeddings: A Conceptual Exercise**\n",
    "\n",
    "Let's make the concept of \"semantic distance\" tangible. We'll embed three sentences and then use a common mathematical measure called **Cosine Similarity** to see how \"close\" they are.\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors.\n",
    "\n",
    "  * If two vectors point in the exact same direction, their similarity is **1** (they are identical in meaning).\n",
    "  * If they are orthogonal (unrelated), their similarity is **0**.\n",
    "  * If they point in opposite directions, their similarity is **-1**.\n",
    "\n",
    "We'll use the `scikit-learn` library for a convenient helper function, and `numpy` for our vector math.\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn numpy\n",
    "```\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# --- Setup our embedding model ---\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "# --- Define our sentences ---\n",
    "sentence1 = \"The cat sat on the mat.\"\n",
    "sentence2 = \"The feline rested on the rug.\" # Very similar to sentence1\n",
    "sentence3 = \"The car drove down the highway.\" # Very different\n",
    "\n",
    "# --- Embed the sentences ---\n",
    "# We can use embed_documents to process them all at once\n",
    "embeddings = hf_embeddings.embed_documents([sentence1, sentence2, sentence3])\n",
    "\n",
    "# The result 'embeddings' is a list of 3 vectors.\n",
    "# Let's convert them to NumPy arrays for easier math.\n",
    "embedding1 = np.array(embeddings[0]).reshape(1, -1)\n",
    "embedding2 = np.array(embeddings[1]).reshape(1, -1)\n",
    "embedding3 = np.array(embeddings[2]).reshape(1, -1)\n",
    "\n",
    "# --- Calculate and Print Similarities ---\n",
    "\n",
    "# Compare the two similar sentences\n",
    "sim_1_2 = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "print(f\"Similarity between '{sentence1}' and '{sentence2}': {sim_1_2:.4f}\")\n",
    "\n",
    "# Compare the two dissimilar sentences\n",
    "sim_1_3 = cosine_similarity(embedding1, embedding3)[0][0]\n",
    "print(f\"Similarity between '{sentence1}' and '{sentence3}': {sim_1_3:.4f}\")\n",
    "\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Similarity between 'The cat sat on the mat.' and 'The feline rested on the rug.': 0.8633\n",
    "Similarity between 'The cat sat on the mat.' and 'The car drove down the highway.': 0.0811\n",
    "```\n",
    "\n",
    "Look at that\\! The similarity score between the two cat-related sentences is extremely high (close to 1.0), while the score between the cat sentence and the car sentence is very low (close to 0).\n",
    "\n",
    "**This is the engine of our RAG system.** When a user asks a question, we will perform exactly this process: embed the query, and then compare its embedding to the embeddings of all our document chunks to find the ones with the highest similarity score.\n",
    "\n",
    "Visually, you can imagine it like this:\n",
    "\n",
    "-----\n",
    "\n",
    "## **7. Best Practices & Troubleshooting for Embeddings**\n",
    "\n",
    "As you start using embeddings, keep these key points in mind:\n",
    "\n",
    "  * **Consistency is King:** You **must** use the exact same embedding model to embed your source documents and to embed your user queries. If you embed your documents with `all-MiniLM-L6-v2` and your query with OpenAI's model, the resulting vectors are in completely different \"semantic spaces.\" The distance between them would be meaningless. This is the most common mistake beginners make.\n",
    "\n",
    "  * **Model Choice Matters:** There is no single \"best\" embedding model for all tasks.\n",
    "\n",
    "      * For general-purpose RAG, models like `all-MiniLM-L6-v2` (for speed) or `text-embedding-3-small` (for performance) are great starting points.\n",
    "      * If you are working with highly specialized or technical documents (e.g., medical research, legal contracts), you might find that models specifically fine-tuned on that type of data perform better. (This is an advanced topic, but good to be aware of).\n",
    "\n",
    "  * **Local vs. API Trade-offs:**\n",
    "\n",
    "      * **API (OpenAI, Google):** Easy to start, no hardware worries, potentially higher performance. But it costs money, requires an internet connection, and you are sending your data to a third party.\n",
    "      * **Local (Hugging Face):** Free, private, and works offline. But it requires a one-time download, consumes your local CPU/RAM (especially during the initial embedding of many documents), and you are responsible for managing the model.\n",
    "\n",
    "  * **Resource Usage:** Don't be surprised if your laptop fan spins up when you run `embed_documents` on thousands of chunks using a local model. It's a computationally intensive task. This is a one-time cost during the \"indexing\" phase of RAG. The subsequent queries are much faster.\n",
    "\n",
    "-----\n",
    "\n",
    "## **8. Connecting to the \"Archives\" Workflow (Updated Diagram)**\n",
    "\n",
    "Let's update our master plan. We've just added a critical transformation step. We now turn our text chunks into meaningful numbers.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Raw Data Sources] --> B{Document Loader};\n",
    "    B --> C[LangChain Documents (Large)];\n",
    "    C --> D{Text Splitter <br/> (Chunking)};\n",
    "    D --> E[LangChain Documents (Chunks)];\n",
    "    E -- \"Convert Text to Numbers\" --> F{Embedding Model};\n",
    "    F --> G[Vector Embeddings (Numerical Representations)];\n",
    "    G -- \"Ready for Fast Searching!\" --> H[<b>Next Up: Vector Store</b>];\n",
    "```\n",
    "\n",
    "This transformation from text chunks (`E`) into vector embeddings (`G`) is the linchpin. It's the step that unlocks semantic search. Our next and final step in building the \"Archive\" (our knowledge base) will be to take these embeddings and load them into a special databaseâ€”a **Vector Store**â€”so we can search them in milliseconds.\n",
    "\n",
    "-----\n",
    "\n",
    "## **9. Key Takeaways**\n",
    "\n",
    ">   * **What are Embeddings?** They are numerical representations (vectors) of text that capture semantic meaning, allowing computers to understand language.\n",
    ">   * **Why RAG Needs Them:** To perform **semantic search**, finding documents based on meaning, not just keywords. This is crucial for handling user queries effectively.\n",
    ">   * **LangChain Interface:** The `Embeddings` class with its `embed_query()` and `embed_documents()` methods provides a standard way to interact with any embedding model.\n",
    ">   * **You Have Choices:** You can use proprietary models via API (like OpenAI) or free, open-source models that run locally (via Hugging Face and `sentence-transformers`).\n",
    ">   * **The Core Principle:** **Semantic Similarity**. Similar text results in vectors that are mathematically \"close.\" Dissimilar text results in vectors that are \"far apart.\"\n",
    ">   * **Consistency is Critical:** Always use the *same* embedding model for indexing your documents and for embedding user queries.\n",
    "\n",
    "-----\n",
    "\n",
    "## **10. Exercises & Thought Experiments**\n",
    "\n",
    "1.  **Practical Similarity:** Take the code from Section 6. Replace the sentences with your own\\! Try two sentences that are perfect paraphrases. Try a sentence and its opposite. Try a question and its answer. Observe how the cosine similarity score changes.\n",
    "\n",
    "2.  **Model Exploration:** Go to the [Hugging Face Model Hub](https://huggingface.co/models) and use the filters on the left to find models for the \"Sentence Similarity\" task. Find a model other than `all-MiniLM-L6-v2`. What is its dimensionality (often listed as \"Dimensions\" in the model card)? How large is the model download?\n",
    "\n",
    "3.  **Connecting to Chunking:** In our last lesson, we discussed `chunk_overlap`. Why might a healthy `chunk_overlap` be particularly important for creating good, meaningful embeddings? (Hint: Think about sentences that get split right in the middle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b6ea2-7184-4067-95e9-0920ebe6ce66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
