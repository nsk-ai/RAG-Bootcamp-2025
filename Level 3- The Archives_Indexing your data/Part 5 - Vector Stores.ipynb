{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45bea3f-4b83-4fd1-82da-69ad69f4cf4d",
   "metadata": {},
   "source": [
    "# **Level 3: The Archives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2aa249-b12a-4737-b714-6e8f54e5ebae",
   "metadata": {},
   "source": [
    "## Part 5: Vector Stores – The Semantic Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e9471-1b89-4a62-860b-1a29c81fd654",
   "metadata": {},
   "source": [
    "Hello everyone, and welcome back\\! In our last few sessions, we've done some incredible work. We've learned how to load raw data from various sources, split it into manageable chunks, and, most importantly, transform those text chunks into rich, numerical representations called **vector embeddings**.\n",
    "\n",
    "This is a huge step. We've converted meaning into math. But this leads us to the next, very practical problem.\n",
    "\n",
    "-----\n",
    "\n",
    "## 1\\. Recap & Bridge from Last Session: The Search Problem, Revisited\n",
    "\n",
    "So, we have these amazing numerical vectors for every single chunk of our documents. Each vector is essentially a coordinate in a high-dimensional \"meaning space.\" That's fantastic.\n",
    "\n",
    "But now, let's think about the scale. What if you have a knowledge base of 10,000 documents, and you split them into one million chunks? You now have **one million vectors**.\n",
    "\n",
    "The core task of Retrieval-Augmented Generation (RAG) is to find the most relevant chunks of text to answer a user's query. In the land of embeddings, this translates to: \"When a user asks a question, we'll turn that question into a vector, and then we need to find the vectors in our collection of one million that are 'closest' or most similar to our query vector.\"\n",
    "\n",
    "So, how do we do that?\n",
    "\n",
    "You might think, \"Well, can't we just store them all in a big Python list?\" We absolutely could. For a query, we could then iterate through every single vector in our list, calculate its similarity (e.g., using cosine similarity) to our query vector, keep track of the top 'k' results, and return them.\n",
    "\n",
    "This approach works perfectly fine for a few hundred, or maybe even a few thousand, vectors. But what happens when we hit one million? Or ten million? This linear, brute-force search becomes incredibly slow and computationally expensive.\n",
    "\n",
    "**Analogy:** Imagine a massive library with a million books, but with no cataloging system. All the books are just piled on the floor. If you want to find a book about \"the history of artificial intelligence,\" your only option is to pick up every single book, read its summary (its embedding), and compare it to your query. You'd be there for years. This is what a linear search on a list of vectors is like.\n",
    "\n",
    "We need a better way. We need a specialized system designed for one purpose: to store and instantly search through billions of vectors. We need a cataloging system for our semantic library.\n",
    "\n",
    "This is where **Vector Stores** come in.\n",
    "\n",
    "-----\n",
    "\n",
    "## 2\\. What are Vector Stores / Vector Databases? (Your Efficient Knowledge Repository)\n",
    "\n",
    "Let's start with a simple definition.\n",
    "\n",
    "> **Simple Definition:** A **Vector Store** (often called a Vector Database) is a specialized database designed to efficiently store, manage, and search large quantities of vector embeddings based on their similarity.\n",
    "\n",
    "Think of it as the high-performance search engine for our RAG system's brain. Its entire architecture is optimized for one core task: finding the nearest neighbors to a query vector at lightning speed.\n",
    "\n",
    "### Core Capabilities\n",
    "\n",
    "Vector Stores are not just simple storage bins. They come with a powerful set of features:\n",
    "\n",
    "1.  **Storage:** This is the most basic function. A vector store holds not just the numerical vector itself, but also the original text chunk (`page_content`) and any `metadata` we associated with it. This is crucial because after finding the most similar *vector*, we need the original *text* to give to the LLM.\n",
    "\n",
    "2.  **Indexing for Fast Search:** This is the magic. Instead of a brute-force search, vector stores use sophisticated indexing algorithms. A common family of these algorithms is called **Approximate Nearest Neighbor (ANN)**.\n",
    "\n",
    "      * **How ANN Works (The Simple Explanation):** Instead of comparing your query vector to *every single other vector*, ANN algorithms cleverly partition the \"vector space\" into neighborhoods. When a query comes in, the algorithm can very quickly identify which neighborhood(s) to search in, drastically reducing the number of comparisons needed. It's \"approximate\" because it prioritizes incredible speed over finding the *perfect* a mathematical guarantee of the absolute closest neighbors, but in practice, the results are almost always exactly what you need and are delivered in milliseconds.\n",
    "\n",
    "      * **Analogy Revisited:** This is the library's cataloging system. You don't look at every book. You go to the computer, type in your topic, and it instantly tells you, \"Go to Section 7, Aisle C, Shelf 4.\" ANN is that system for vectors.\n",
    "\n",
    "3.  **Similarity Search:** The primary way we interact with a vector store is by performing a similarity search. We provide a query vector, and the store returns the 'k' most similar stored vectors (along with their content and metadata).\n",
    "\n",
    "### Why \"Database\"?\n",
    "\n",
    "We call them databases because, like traditional SQL or NoSQL databases, they handle persistent data management. They provide APIs for **CRUD** operations (Create, Read, Update, Delete), can scale to handle massive loads, and ensure that your data is safely stored and can be retrieved later.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Key Takeaway Box**\n",
    "\n",
    "  * **Problem:** Searching a simple list of millions of vectors is too slow for real-world applications.\n",
    "  * **Solution:** **Vector Stores** are specialized databases built for this exact problem.\n",
    "  * **Core Function:** They use clever indexing algorithms (like **Approximate Nearest Neighbor - ANN**) to perform incredibly fast similarity searches.\n",
    "  * **What they store:** Vectors, the original text content, and associated metadata.\n",
    "\n",
    "-----\n",
    "\n",
    "-----\n",
    "\n",
    "## 3\\. Why Do We Need Vector Stores for RAG? (The Engine of Retrieval)\n",
    "\n",
    "Now that we know *what* they are, let's connect this directly to our RAG systems. Why are they a non-negotiable component?\n",
    "\n",
    "  * **Scalability:** As we discussed, RAG systems are only as good as the knowledge they can access. Vector stores allow us to scale our knowledge base from a few pages to millions of documents without our application grinding to a halt.\n",
    "\n",
    "  * **Speed:** Users expect real-time answers. A RAG system needs to perform the `Retrieval -> Augmentation -> Generation` loop in seconds. The retrieval step, which can be the slowest, is made possible in milliseconds by a vector store. This is the difference between a practical, interactive AI and a research prototype.\n",
    "\n",
    "  * **Enabling Semantic Search:** Vector stores are the infrastructure that makes \"search by meaning\" a practical reality. They are the engines that power the semantic search at the heart of every RAG system.\n",
    "\n",
    "  * **Metadata Filtering (A Superpower):** This is one of the most powerful and often overlooked features. A good vector store doesn't just search by vector similarity. It allows you to **pre-filter** the search space based on the metadata you stored with your chunks.\n",
    "\n",
    "      * **Example:** Imagine your knowledge base contains documents from HR, Finance, and Engineering, spanning the years 2020-2025. You can ask a query like: \"What were the key project milestones?\" and add a filter to **only search within documents from the 'Engineering' department created in '2023'**. This dramatically improves accuracy by preventing the system from retrieving irrelevant documents from other departments or years, even if their vectors are semantically similar.\n",
    "\n",
    "-----\n",
    "\n",
    "## 4\\. Introducing LangChain's VectorStore Integration\n",
    "\n",
    "One of the best things about LangChain is its role as a standardizing framework. This is especially true for vector stores. There are dozens of different vector stores available, each with its own specific API.\n",
    "\n",
    "Instead of forcing you to learn a new API for every different vector store, LangChain provides a **standard `VectorStore` interface**.\n",
    "\n",
    "This is a huge advantage. It means you can build your entire application using the standard LangChain interface, and if you later decide to switch from one vector store (e.g., a local one for development) to another (e.g., a cloud-based one for production), you only need to change a few lines of code where you initialize the store. The rest of your application logic remains the same.\n",
    "\n",
    "### Key LangChain Methods\n",
    "\n",
    "You'll quickly become very familiar with these core methods on any LangChain `VectorStore` object:\n",
    "\n",
    "1.  **`from_documents()`:** This is a class method and your primary tool for creating a vector store from scratch. It's a powerhouse that does several things in one call:\n",
    "\n",
    "      * Takes your list of `Document` chunks.\n",
    "      * Takes an `Embeddings` model instance.\n",
    "      * For each chunk, it calls the embedding model to create a vector.\n",
    "      * It then adds the chunks (content + metadata) and their corresponding vectors to the vector store.\n",
    "      * It handles all the indexing for you.\n",
    "\n",
    "2.  **`add_documents()`:** If you already have an existing vector store and want to add new documents to it, this is the method you'll use.\n",
    "\n",
    "3.  **`similarity_search()`:** This is the star of the show for retrieval. You pass it a simple query string. LangChain automatically takes that string, uses the same embedding model you initialized the store with to turn it into a query vector, and then performs the similarity search in the vector store. It returns a list of the `k` most similar `Document` objects.\n",
    "\n",
    "Let's see this in action with a concrete example.\n",
    "\n",
    "-----\n",
    "\n",
    "## 5\\. Deep Dive: ChromaDB – Our Chosen Vector Store\n",
    "\n",
    "For our journey, we'll start with **ChromaDB**. It's an excellent choice for several reasons:\n",
    "\n",
    "  * **Open-Source & Developer-Friendly:** It's free to use and has a very active community.\n",
    "  * **Easy to Start:** It's incredibly simple to get running. You can run it entirely in-memory for quick scripts or have it persist to a local disk directory with a single parameter change.\n",
    "  * **Excellent LangChain Integration:** It's one of the most well-supported vector stores in the LangChain ecosystem.\n",
    "  * **Scalable:** While it's great for learning, Chroma can also be run as a client-server application, making it suitable for more serious projects.\n",
    "\n",
    "### Installation\n",
    "\n",
    "First things first, let's get it installed.\n",
    "\n",
    "```bash\n",
    "pip install chromadb\n",
    "```\n",
    "\n",
    "### Initializing and Using ChromaDB\n",
    "\n",
    "Let's write a complete example that ties together everything from our last few lessons. We'll create some document chunks, initialize an embedding model, and then store and query them using ChromaDB.\n",
    "\n",
    "First, let's set up our environment and sample data. We'll use `OpenAIEmbeddings` for this example, so make sure your API key is set up.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# --- 1. Set up Environment ---\n",
    "# Make sure you have your OPENAI_API_KEY set in your environment variables\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# --- 2. Prepare Sample Data (Documents) ---\n",
    "# In a real scenario, these would come from a Document Loader and Text Splitter.\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"The cat sat on the mat. It was a fluffy, white cat.\",\n",
    "        metadata={\"source\": \"cat_facts.txt\", \"chapter\": 1},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"The dog chased the ball in the park. It was a golden retriever.\",\n",
    "        metadata={\"source\": \"dog_stories.txt\", \"chapter\": 1},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Artificial intelligence is transforming industries, from healthcare to finance.\",\n",
    "        metadata={\"source\": \"ai_trends.pdf\", \"page\": 12},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A transformer model is a neural network architecture that relies on self-attention mechanisms.\",\n",
    "        metadata={\"source\": \"ai_deep_dive.pdf\", \"page\": 4},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# --- 3. Initialize Embeddings Model ---\n",
    "# We will use the same model to embed our documents and our queries.\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# --- 4. Initialize ChromaDB and Add Documents ---\n",
    "# This is the core step. `from_documents` handles everything:\n",
    "# - It embeds each document using the provided model.\n",
    "# - It stores the original document content, metadata, and the new embedding.\n",
    "# This creates an IN-MEMORY instance of Chroma. The data will be lost when the script ends.\n",
    "print(\"Creating in-memory Chroma vector store...\")\n",
    "vector_store_in_memory = Chroma.from_documents(docs, embeddings_model)\n",
    "print(\"In-memory store created.\")\n",
    "\n",
    "# --- 5. Perform a Similarity Search ---\n",
    "query = \"What is the core of a transformer network?\"\n",
    "print(f\"\\nPerforming similarity search for query: '{query}'\")\n",
    "\n",
    "# The `similarity_search` method embeds the query and finds the most similar documents.\n",
    "retrieved_docs = vector_store_in_memory.similarity_search(query, k=2)\n",
    "\n",
    "print(\"\\n--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"  Content: {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "```\n",
    "\n",
    "When you run this, you will see that the most relevant documents about AI and transformers are retrieved, not the ones about cats and dogs. This is semantic search in action\\!\n",
    "\n",
    "### Persisting Your Vector Store\n",
    "\n",
    "The in-memory version is great for quick tests, but what if you want to index your data once and then query it many times without re-processing everything? You need to persist the data to disk.\n",
    "\n",
    "This is incredibly easy with Chroma. You just need to specify a `persist_directory`.\n",
    "\n",
    "```python\n",
    "# --- 6. Creating and Persisting a Vector Store to Disk ---\n",
    "persist_directory = \"./chroma_db_persistent\"\n",
    "\n",
    "print(f\"\\nCreating persistent Chroma vector store at: {persist_directory}\")\n",
    "\n",
    "# By adding a `persist_directory`, Chroma will save the files here.\n",
    "vector_store_persistent = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "print(\"Persistent store created and saved.\")\n",
    "\n",
    "# The data is now on your disk. Let's imagine our program ended.\n",
    "# Now, in a new script, we can load this database without reprocessing the documents.\n",
    "\n",
    "# --- 7. Loading a Persistent Vector Store ---\n",
    "print(\"\\nLoading persistent store from disk...\")\n",
    "# Note: We just need to provide the directory and the embedding function.\n",
    "# The documents are already stored and embedded in the database.\n",
    "loaded_vector_store = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings_model\n",
    ")\n",
    "print(\"Persistent store loaded.\")\n",
    "\n",
    "\n",
    "# --- 8. Querying the Loaded Store ---\n",
    "query_persistent = \"Tell me about animal behavior.\"\n",
    "print(f\"\\nPerforming similarity search on loaded store for query: '{query_persistent}'\")\n",
    "\n",
    "retrieved_docs_persistent = loaded_vector_store.similarity_search(query_persistent, k=2)\n",
    "\n",
    "print(\"\\n--- Retrieved Documents (from loaded store) ---\")\n",
    "for i, doc in enumerate(retrieved_docs_persistent):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"  Content: {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 20)\n",
    "```\n",
    "\n",
    "As you can see, saving and loading is trivial. This workflow is fundamental for any real application. You'll run your indexing pipeline (load, split, embed, store) once, and then your live application will simply load the persistent store to perform queries.\n",
    "\n",
    "-----\n",
    "\n",
    "## 6\\. Other Vector Store Options\n",
    "\n",
    "While Chroma is fantastic, the ecosystem is vast. LangChain's standard interface makes it easy to try others. Let's briefly look at two other popular options to understand the landscape.\n",
    "\n",
    "### FAISS (Facebook AI Similarity Search)\n",
    "\n",
    "  * **Overview:** FAISS is not a full-fledged database; it's a highly optimized **library** for similarity search, primarily designed to run in-memory.\n",
    "  * **Pros:** Extremely fast for local, in-memory operations. It's often the quickest choice for prototyping and experiments that fit within your machine's RAM.\n",
    "  * **Cons:** It's not a persistent database by default. You have to manually save the index to a file and load it back. It doesn't have the database features like a client-server model or advanced metadata filtering that Chroma offers.\n",
    "  * **Code Example:**\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- FAISS Example ---\n",
    "# 1. Create the in-memory index (very similar to Chroma)\n",
    "print(\"\\nCreating FAISS index...\")\n",
    "faiss_index = FAISS.from_documents(docs, embeddings_model)\n",
    "print(\"FAISS index created.\")\n",
    "\n",
    "# 2. Perform a search\n",
    "query_faiss = \"What is artificial intelligence about?\"\n",
    "retrieved_faiss = faiss_index.similarity_search(query_faiss, k=1)\n",
    "print(f\"\\nFAISS Retrieved Content: '{retrieved_faiss[0].page_content}'\")\n",
    "\n",
    "# 3. Save and load the index\n",
    "faiss_index.save_local(\"my_faiss_index\")\n",
    "loaded_faiss_index = FAISS.load_local(\"my_faiss_index\", embeddings_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 4. Test the loaded index\n",
    "retrieved_loaded = loaded_faiss_index.similarity_search(query_faiss, k=1)\n",
    "print(f\"Loaded FAISS Retrieved Content: '{retrieved_loaded[0].page_content}'\")\n",
    "```\n",
    "\n",
    "**Pro Tip:** Use FAISS when you need the absolute fastest *local* search and don't need advanced database features. It's excellent for temporary, script-based workflows.\n",
    "\n",
    "### Pinecone (Managed Cloud Vector Database)\n",
    "\n",
    "  * **Overview:** Pinecone is a leading **managed cloud service** for vector databases. This means you don't run the database on your own machine; you interact with it over the internet via an API. They handle all the infrastructure, scaling, and maintenance for you.\n",
    "  * **Pros:** Production-ready and highly scalable to billions of vectors. Very low latency. You pay for what you use and don't have to worry about managing servers.\n",
    "  * **Cons:** It's a cloud service, so it requires an internet connection and an API key. It also incurs costs, unlike running Chroma or FAISS locally. It adds an external dependency to your application.\n",
    "  * **Code Example (Conceptual / Setup Focus):**\n",
    "    *To run this, you would need to install `pinecone-client` and get an API key from the Pinecone website.*\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain_pinecone import Pinecone\n",
    "\n",
    "# --- Pinecone Conceptual Example ---\n",
    "# You would need to:\n",
    "# 1. `pip install pinecone-client`\n",
    "# 2. Sign up on the Pinecone website to get an API key and index name.\n",
    "\n",
    "# os.environ[\"PINECONE_API_KEY\"] = \"YOUR_PINECONE_API_KEY\"\n",
    "\n",
    "# The main difference is the initialization. You point it to your cloud index.\n",
    "# index_name = \"my-langchain-rag-index\"\n",
    "\n",
    "# print(\"\\nConnecting to Pinecone...\")\n",
    "# pinecone_vector_store = Pinecone.from_documents(\n",
    "#     docs, embeddings_model, index_name=index_name\n",
    "# )\n",
    "# print(\"Documents uploaded to Pinecone.\")\n",
    "\n",
    "# Searching is the same standard LangChain interface!\n",
    "# retrieved_pinecone = pinecone_vector_store.similarity_search(\n",
    "#     \"What is AI?\", k=1\n",
    "# )\n",
    "# print(retrieved_pinecone)\n",
    "```\n",
    "\n",
    "**Crucial Point:** The key difference here isn't the code's complexity, but the **operational model**. Chroma and FAISS (in our examples) are local. Pinecone is a managed cloud service. You choose a cloud service when your project needs to be deployed for others to use and requires high availability and scalability beyond what a single local machine can offer.\n",
    "\n",
    "### Other Alternatives\n",
    "\n",
    "The world of vector stores is rich and growing. For your awareness, here are other excellent options, all supported by LangChain: **Weaviate, Qdrant, Milvus, Supabase (using the pgvector extension), Redis**, and more. Each has its own unique strengths. The beauty of the LangChain interface is that you can learn the core concepts once and apply them across many different backends.\n",
    "\n",
    "-----\n",
    "\n",
    "## 7\\. Best Practices & Troubleshooting\n",
    "\n",
    "As you start working with vector stores, keep these key points in mind. Trust me, remembering these will save you hours of debugging.\n",
    "\n",
    "  * **Embeddings Consistency:** This is the golden rule. You **must** use the exact same embedding model to create the vectors for your documents and to create the vector for your query. If you use different models, the vectors will not be in the same \"meaning space,\" and your search results will be meaningless.\n",
    "  * **Persistence is Key:** For any project that isn't a throwaway script, use a persistent vector store. Use Chroma's `persist_directory` or FAISS's `save_local()` method. You don't want to re-embed your entire knowledge base every time you run your app.\n",
    "  * **Leverage Metadata:** Don't just throw text into your documents. Think about what metadata would be useful for filtering later. What is the source? What's the date? Who is the author? Which department does it belong to? Adding this context upfront gives you superpowers during retrieval.\n",
    "  * **Tune Your `k`:** The `k` parameter in `similarity_search(query, k=N)` controls how many documents are returned. If `k` is too low, you might miss relevant context. If it's too high, you might overwhelm your LLM with too much, potentially irrelevant, information. Finding the right `k` is a key part of tuning your RAG system.\n",
    "  * **Local vs. Cloud:** Start with local vector stores like Chroma for development and prototyping. They are free and fast to set up. Move to a managed cloud solution like Pinecone when you are ready to deploy a scalable, production-grade application.\n",
    "\n",
    "-----\n",
    "\n",
    "## 8\\. The Complete Indexing & Retrieval Workflow\n",
    "\n",
    "Let's zoom out and look at the entire picture. The Vector Store is the central component that connects our data preparation (indexing) with our question-answering (retrieval) process.\n",
    "\n",
    "### The Full \"R\" in RAG\\!\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Indexing Pipeline (Done once or periodically)\n",
    "        A[Raw Data Sources] --> B{Document Loader};\n",
    "        B --> C[LangChain Documents (Large)];\n",
    "        C --> D{Text Splitter <br/> (Chunking)};\n",
    "        D --> E[LangChain Documents (Chunks)];\n",
    "        E -- \"Convert to Numerical Form\" --> F{Embedding Model};\n",
    "        F --> G[Vector Embeddings (Numbers!)];\n",
    "        G -- \"Store for Fast Search\" --> H[<br><b>Vector Store</b><br> (Chroma, FAISS, Pinecone, etc.)];\n",
    "    end\n",
    "\n",
    "    subgraph Retrieval Pipeline (Done for every query)\n",
    "        I[User Query] --> J{Embedding Model};\n",
    "        J --> K[Query Vector];\n",
    "        K -- \"Similarity Search (k=N)\" --> H;\n",
    "        H -- \"Returns Relevant\" --> L[Retrieved Chunks];\n",
    "        L -- \"Used as Context\" --> M[LLM <br/> (for Generation)];\n",
    "        M --> N[Answer];\n",
    "    end\n",
    "```\n",
    "\n",
    "Look at this diagram carefully. We have now built almost this entire system\\! The **Vector Store (H)** is the critical bridge. Everything in the \"Indexing Pipeline\" is about preparing and storing knowledge *in* the vector store. The \"Retrieval Pipeline\" is about getting knowledge *out* of the vector store to help the LLM generate a better answer.\n",
    "\n",
    "We have successfully built the \"Retrieval\" engine. In our next lecture, we will finally connect this retriever to a prompt and an LLM to complete the full RAG chain.\n",
    "\n",
    "-----\n",
    "\n",
    "## 9\\. Key Takeaways\n",
    "\n",
    "  * **Vector Stores** are the solution to storing and efficiently searching millions or billions of vector embeddings.\n",
    "  * They use **Approximate Nearest Neighbor (ANN)** algorithms to provide search results in milliseconds, making real-time RAG applications possible.\n",
    "  * **LangChain** provides a standardized `VectorStore` interface, allowing you to swap backends (like Chroma, FAISS, Pinecone) with minimal code changes.\n",
    "  * **ChromaDB** is an excellent, open-source choice for getting started, offering both in-memory and persistent local storage.\n",
    "  * **FAISS** is a super-fast in-memory library, great for local prototyping.\n",
    "  * **Pinecone** is a leading managed cloud solution, built for production scale and performance.\n",
    "  * Always use the **same embedding model** for indexing and querying.\n",
    "  * Use **metadata** to enable powerful filtering that dramatically improves search relevance.\n",
    "  * The Vector Store is the central archive of your RAG system's knowledge.\n",
    "\n",
    "-----\n",
    "\n",
    "## 10\\. Exercises & Thought Experiments\n",
    "\n",
    "1.  **Personal Knowledge Base:** Take a few pages of your own personal notes (from a `.txt` file, or just copy-paste them). Use a `RecursiveCharacterTextSplitter` to chunk them, embed them with `OpenAIEmbeddings`, and store them in a persistent ChromaDB instance. Now, ask it questions about your own notes. See how well it retrieves the relevant passages.\n",
    "\n",
    "2.  **Metadata Power:** Take the code from exercise 1. Before creating the `Document` objects, manually add some metadata. For example: `metadata={\"topic\": \"work\"}` or `metadata={\"topic\": \"personal_ideas\", \"priority\": \"high\"}`. Load these into Chroma. LangChain's Chroma wrapper has a `similarity_search` method that can accept a `filter` argument. Research how to use it and try to perform a search that only looks at documents with a specific metadata tag. *Hint: The `filter` argument in Chroma's API often takes a dictionary like `{\"topic\": \"work\"}`*.\n",
    "\n",
    "3.  **Prototyping vs. Production:** Discuss with a partner (or write down your thoughts): When would you choose to start a project with FAISS versus starting with ChromaDB? At what point in a project's lifecycle would you consider migrating from a local ChromaDB instance to a managed cloud service like Pinecone? What factors would influence that decision (e.g., number of users, size of data, need for availability)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4942a1-cf2f-4033-a9fd-340430e8db8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
