{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763fd9f1-10be-4c43-98b7-6dc5f8a3f404",
   "metadata": {},
   "source": [
    "# **Level 4: The Quest: Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e646ef0-7cac-4a3b-b825-ac7313ef54fe",
   "metadata": {},
   "source": [
    "## Part 4: Dense Search – Finding Meaning in Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25835a-cb79-48ff-a53f-db4002dc898b",
   "metadata": {},
   "source": [
    "\n",
    "Hello everyone, and welcome back to our journey through the world of Retrieval-Augmented Generation. We've spent a lot of time building our \"Archives\" — we've loaded documents, chopped them into manageable chunks, and, most importantly, we've created **embeddings**, those powerful numerical fingerprints that capture the *meaning* of our text. We've stored these embeddings in our Vector Store, our specialized, high-speed library.\n",
    "\n",
    "Now, the real quest begins. We have this beautifully organized, meaning-encoded library. The crucial question is: How do we actually *use* it? How do we ask a question in plain English and get the most relevant pieces of knowledge back?\n",
    "\n",
    "This is where **Dense Search** comes in. It's the engine that powers our quest, the librarian that understands not just the words you say, but the meaning behind them.\n",
    "\n",
    "-----\n",
    "\n",
    "## 1\\. Recap and Bridge: From Numbers to Meaning\n",
    "\n",
    "Let's quickly reconnect with what we've already mastered:\n",
    "\n",
    "  * **Chunks:** We know that we can't feed entire books to a Large Language Model (LLM). So, we break our documents down into bite-sized, semantically coherent pieces of text called chunks.\n",
    "  * **Embeddings:** We learned that an embedding model (like those from OpenAI, Hugging Face, or Google) acts as a \"universal translator,\" turning our text chunks—and our future queries—into numerical vectors. These aren't random numbers; they are coordinates in a high-dimensional \"meaning space.\"\n",
    "  * **Vector Stores:** We've seen how specialized databases like ChromaDB and FAISS are designed to store these embedding vectors and, critically, to search through them with incredible speed.\n",
    "\n",
    "So, we have chunks of text, each with a corresponding vector fingerprint, all stored neatly in our Vector Store.\n",
    "\n",
    "**The Core Problem:** How do we actually *use* those embeddings and our Vector Store to find information based on *meaning*, not just exact words? If a user asks, \"How do I secure my online account?\", we need our system to find a document that says, \"Steps to reset your password,\" even though the words \"secure,\" \"online,\" and \"account\" don't appear in that exact phrase.\n",
    "\n",
    "This is the magic of **Dense Search**. It's the process of using vector embeddings to find semantically similar information. It's the engine for \"search by meaning,\" and it's what separates a basic keyword search from a truly intelligent RAG system.\n",
    "\n",
    "You'll recall from our overview of retrieval types that keyword (or sparse) search is excellent for finding exact matches. If you need to find every document that contains the specific term \"ISO-27001,\" keyword search is your best friend. But for the vast majority of user queries, which are conversational and conceptual, Dense Search is the indispensable tool.\n",
    "\n",
    "-----\n",
    "\n",
    "## 2\\. What is Dense Search? The Semantic Superpower Unleashed\n",
    "\n",
    "At its heart, Dense Search is surprisingly intuitive. It operates on a simple, yet powerful, principle: **similar meanings live in the same neighborhood.**\n",
    "\n",
    "### The Vector Space\n",
    "\n",
    "Imagine a massive, invisible space with hundreds or even thousands of dimensions. When we create an embedding for a piece of text, we're placing it at a specific coordinate within this space. The embedding model is trained in such a way that texts with similar meanings—like \"common cold symptoms\" and \"runny nose and sore throat\"—are placed very close to each other. Texts with different meanings—like \"common cold symptoms\" and \"stock market analysis\"—are placed very far apart.\n",
    "\n",
    "### How it Works (Step-by-Step)\n",
    "\n",
    "Dense Search is the process of navigating this space to find the nearest neighbors to our query. Here’s the play-by-play:\n",
    "\n",
    "1.  **Query Embedding:** The user's natural language query (e.g., \"What are the best ways to stay healthy?\") is fed into the *exact same embedding model* that we used to create our document chunk embeddings. This is a critical point—using different models would be like trying to find a location using two completely different maps. The output is a **query vector**.\n",
    "2.  **Similarity Calculation:** The Vector Store takes this query vector and efficiently calculates the mathematical \"distance\" or \"similarity\" between it and all the document chunk vectors it holds. The most common way to measure this \"closeness\" is a metric called **Cosine Similarity**. You don't need to know the formula, just the concept: it measures the angle between two vectors. A smaller angle means the vectors are pointing in a similar direction, indicating higher semantic similarity, regardless of their magnitude.\n",
    "3.  **Top-K Retrieval:** The Vector Store then returns the `k` document chunks whose vectors are most similar (i.e., \"closest\") to the query vector. `k` is simply the number of results you want back.\n",
    "\n",
    "Here’s a simple diagram to visualize this flow:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User Query (\"How to protect my account?\")] --> B{Embedding Model};\n",
    "    B --> C[Query Vector ([0.1, 0.9, ...])];\n",
    "    C --> D{Vector Store};\n",
    "    D -- \"Calculates Similarity (e.g., Cosine)\" --> E[Stored Document Vectors];\n",
    "    E -- \"Returns Top K Closest\" --> F[Relevant Document Chunks (e.g., \"Steps to reset your password...\")];\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## 3\\. Why Dense Search is Indispensable for RAG\n",
    "\n",
    "So, why go through all this trouble of creating vectors and calculating distances? Because Dense Search gives our RAG system a form of \"understanding\" that is simply impossible with older search methods.\n",
    "\n",
    "  * **Semantic Understanding:** This is the primary benefit. Dense Search gracefully handles the nuances of human language.\n",
    "\n",
    "      * **Synonyms:** A query for \"fast car\" can easily find a document about a \"swift automobile.\"\n",
    "      * **Paraphrases:** A query like \"How do I reset my account?\" is semantically very close to a document chunk that says, \"Follow these steps to change your password.\" Dense search bridges this gap effortlessly.\n",
    "      * **Conceptual Queries:** This is where it truly shines. A user can ask a high-level question like, \"What are the best practices for energy conservation?\" and the system can retrieve documents discussing \"reducing your home's carbon footprint,\" \"sustainable living tips,\" and \"improving appliance efficiency,\" because the *concepts* are related.\n",
    "\n",
    "  * **Robustness:** Your users don't need to be search experts. They don't have to guess the exact keywords that might be in your documents. They can ask questions naturally, and the system is robust enough to find relevant information even if the phrasing is completely different.\n",
    "\n",
    "  * **Improved User Experience:** The result is a far more natural and intuitive interaction. It feels less like searching a database and more like having a conversation with an expert who understands what you're *really* asking for.\n",
    "\n",
    "  * **Handles Polysemy (Words with Multiple Meanings):** The context of a sentence is baked into its embedding. For example, the embedding for \"the bank of the river\" will be in a completely different part of the vector space than the embedding for \"the bank approved my loan.\" When a user's query provides context, dense search can retrieve documents related to the *correct* meaning of the word.\n",
    "\n",
    "-----\n",
    "\n",
    "## 4\\. Implementing Dense Search in LangChain: The `VectorStoreRetriever`\n",
    "\n",
    "Alright, let's move from theory to practice. In LangChain, the workhorse for performing dense search is the `VectorStoreRetriever`. We've touched on it before, but now we're going to dive deep into its most important configurations.\n",
    "\n",
    "You create a retriever directly from an initialized `VectorStore` object using the `.as_retriever()` method. The real power lies in the parameters you can pass to this method.\n",
    "\n",
    "### Key Configuration Parameters\n",
    "\n",
    "#### `search_type=\"similarity\"` (The Default)\n",
    "\n",
    "This is your standard, bread-and-butter dense search. It performs the \"Top-K\" retrieval we just discussed: find the `k` documents most semantically similar to the query. If you don't specify a `search_type`, this is what you get.\n",
    "\n",
    "#### `search_type=\"mmr\"` (Maximal Marginal Relevance)\n",
    "\n",
    "Let's start with the problem MMR solves. Imagine you have several document chunks that are very similar to each other—perhaps they are slight rephrasings of the same company policy. If you do a standard similarity search, you might get back four or five of these highly redundant chunks. This isn't very helpful for the LLM; it's just seeing the same information over and over.\n",
    "\n",
    "**Maximal Marginal Relevance (MMR)** is a clever algorithm designed to solve this by promoting **diversity** in your search results.\n",
    "\n",
    "  * **How it Works (Simplified):** MMR first fetches a larger set of documents that are relevant to the query (this is the `fetch_k` parameter). Then, it iteratively selects documents from that set to be in the final results. In each step, it picks the document that best balances two things:\n",
    "\n",
    "    1.  Being relevant to the original query.\n",
    "    2.  Being *different* from the documents it has *already selected*.\n",
    "\n",
    "  * **Analogy:** You ask a friend for \"recommendations for Italian food.\" A standard similarity search might give you three different lasagna recipes. An MMR search would be more likely to give you one lasagna recipe, one carbonara recipe, and one risotto recipe—a more diverse and useful set of results.\n",
    "\n",
    "  * **Key Parameters for MMR:**\n",
    "\n",
    "      * `k`: The final number of documents you want the retriever to return.\n",
    "      * `fetch_k`: The initial number of candidate documents to retrieve from the vector store before the MMR algorithm does its diversity ranking. To give MMR enough options to choose from, **`fetch_k` should be larger than `k`**, often 2-4 times larger.\n",
    "\n",
    "#### `search_kwargs`: The Control Panel for Your Search\n",
    "\n",
    "This is a dictionary that allows you to pass specific arguments directly to the underlying vector store's search method. This is how you control the core behavior of the search itself.\n",
    "\n",
    "  * **`k` (Number of Results):** This is the most common and important argument. It dictates how many top-similar documents you want back.\n",
    "\n",
    "      * **Impact:** Choosing the right `k` is a balancing act. Too small a `k` (`k=1` or `k=2`) might cause you to miss crucial context spread across multiple chunks. Too large a `k` (`k=10` or `k=20`) might introduce noise and less relevant information, potentially confusing the LLM or exceeding its context window. A value between 3 and 5 is often a good starting point.\n",
    "\n",
    "  * **`filter` (Metadata Filtering):** This is an incredibly powerful feature. It allows you to combine the \"fuzzy\" semantic search with the \"precise\" logic of metadata filters. You can tell the retriever to *only* perform the semantic search on a subset of your documents that match specific metadata criteria.\n",
    "\n",
    "    For example, you could search for the concept \"annual performance\" but only within documents where `{\"source\": \"financial_reports.pdf\"}` and `{\"year\": 2023}`. This dramatically improves accuracy by pre-filtering the search space.\n",
    "\n",
    "-----\n",
    "\n",
    "## 5\\. Practical Code Examples: Hands-On Dense Search\n",
    "\n",
    "Time to get our hands dirty. Let's see these concepts in action.\n",
    "\n",
    "### Prerequisites Setup\n",
    "\n",
    "First, make sure you have the necessary libraries installed.\n",
    "\n",
    "```bash\n",
    "pip install -qU langchain langchain-openai langchain-community chromadb faiss-cpu sentence-transformers\n",
    "```\n",
    "\n",
    "Next, let's set up our environment with some sample documents, an embedding model, and a vector store.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# For our examples, we'll use a free, local Sentence Transformer model.\n",
    "# If you have an OpenAI API key, you could use `from langchain_openai import OpenAIEmbeddings`\n",
    "# and set `embeddings = OpenAIEmbeddings()`.\n",
    "\n",
    "# 1. Initialize our Embedding Model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "print(\"Embedding model initialized.\")\n",
    "\n",
    "# 2. Define our Sample Documents with varied content and metadata\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"The new CEO, Sarah Jenkins, announced a company-wide focus on AI integration.\",\n",
    "        metadata={\"source\": \"press-release\", \"year\": 2024, \"author\": \"CorpComms\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Our quarterly report shows a 15% growth in the tech sector, largely driven by AI advancements.\",\n",
    "        metadata={\"source\": \"financial-report\", \"year\": 2024, \"author\": \"FinanceDept\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"To reset your password, please visit the account portal and click 'Forgot Password'.\",\n",
    "        metadata={\"source\": \"faq\", \"topic\": \"account-management\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"If you are unable to access your account, please follow the password recovery steps.\",\n",
    "        metadata={\"source\": \"faq\", \"topic\": \"account-management\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"The arctic fox is known for its thick, white fur that provides camouflage in the snow.\",\n",
    "        metadata={\"source\": \"nature-facts\", \"topic\": \"animals\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A swift, reddish-brown canine leaps over a slumbering dog in the classic pangram.\",\n",
    "        metadata={\"source\": \"literature\", \"topic\": \"animals\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. Create our Vector Store (using ChromaDB for this example)\n",
    "# We'll use a persistent directory to store our DB.\n",
    "CHROMA_DB_DIR = \"./chroma_db_dense_search\"\n",
    "db = Chroma.from_documents(docs, embeddings, persist_directory=CHROMA_DB_DIR)\n",
    "print(f\"ChromaDB initialized and populated at {CHROMA_DB_DIR}\")\n",
    "```\n",
    "\n",
    "### Example 1: Basic `similarity_search`\n",
    "\n",
    "This is our baseline. Let's perform a standard semantic search and see how it handles synonyms and paraphrased questions.\n",
    "\n",
    "```python\n",
    "# Create a standard similarity retriever\n",
    "retriever_similarity = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2} # We want the top 2 most similar documents\n",
    ")\n",
    "\n",
    "# --- Test Case 1: Paraphrased Query ---\n",
    "query_paraphrase = \"How can I regain access to my account?\"\n",
    "retrieved_docs_para = retriever_similarity.invoke(query_paraphrase)\n",
    "\n",
    "print(f\"\\n--- Basic Similarity Search for: '{query_paraphrase}' ---\")\n",
    "for i, doc in enumerate(retrieved_docs_para):\n",
    "    print(f\"Doc {i+1}: Content='{doc.page_content}'\")\n",
    "    print(f\"   Source='{doc.metadata.get('source')}', Topic='{doc.metadata.get('topic')}'\\n\")\n",
    "\n",
    "# --- Test Case 2: Conceptual Query ---\n",
    "query_conceptual = \"What's the latest news on artificial intelligence in the company?\"\n",
    "retrieved_docs_concept = retriever_similarity.invoke(query_conceptual)\n",
    "\n",
    "print(f\"\\n--- Basic Similarity Search for: '{query_conceptual}' ---\")\n",
    "for i, doc in enumerate(retrieved_docs_concept):\n",
    "    print(f\"Doc {i+1}: Content='{doc.page_content}'\")\n",
    "    print(f\"   Source='{doc.metadata.get('source')}', Year='{doc.metadata.get('year')}'\\n\")\n",
    "```\n",
    "\n",
    "**Expected Output Analysis:**\n",
    "\n",
    "For the first query, you'll notice it correctly retrieves the two documents about password recovery, even though our query used different words like \"regain access.\" For the second query, it correctly identifies the press release and financial report as being conceptually related to \"AI in the company.\" This is the power of dense search.\n",
    "\n",
    "-----\n",
    "\n",
    "### Example 2: Using `search_type=\"mmr\"` for Diversity\n",
    "\n",
    "Now, let's look at a scenario where a standard search might give us redundant results. Our two FAQ documents are very similar. Let's craft a query that retrieves them and see how MMR can give us a more diverse set of results.\n",
    "\n",
    "```python\n",
    "# Create an MMR retriever\n",
    "mmr_retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 2, \"fetch_k\": 4} # Fetch 4, but return the best 2 diverse results\n",
    ")\n",
    "\n",
    "query_mmr = \"Tell me about account help and animals.\" # A broad query to test diversity\n",
    "\n",
    "# First, let's see what the standard similarity retriever does\n",
    "retrieved_docs_sim = retriever_similarity.invoke(query_mmr)\n",
    "print(f\"\\n--- Standard Similarity Search Results for: '{query_mmr}' ---\")\n",
    "for i, doc in enumerate(retrieved_docs_sim):\n",
    "    print(f\"Doc {i+1}: Content='{doc.page_content}'\")\n",
    "    print(f\"   Topic='{doc.metadata.get('topic')}'\\n\")\n",
    "\n",
    "# Now, let's see what the MMR retriever does\n",
    "retrieved_docs_mmr = mmr_retriever.invoke(query_mmr)\n",
    "print(f\"\\n--- MMR Search Results for: '{query_mmr}' ---\")\n",
    "print(\"Note how MMR tries to pick one from each topic.\")\n",
    "for i, doc in enumerate(retrieved_docs_mmr):\n",
    "    print(f\"Doc {i+1}: Content='{doc.page_content}'\")\n",
    "    print(f\"   Topic='{doc.metadata.get('topic')}'\\n\")\n",
    "```\n",
    "\n",
    "**Expected Output Analysis:**\n",
    "\n",
    "With the standard `similarity` search, you are likely to get back the two highly similar \"account-management\" documents because they are both very close semantically to \"account help.\"\n",
    "\n",
    "With the `mmr` search, the algorithm will likely pick one of the \"account-management\" documents (because it's highly relevant) and then, instead of picking the *other* very similar account document, it will pick one of the \"animals\" documents because it's also relevant to the query but adds more diversity to the result set.\n",
    "\n",
    "> #### **Key Takeaway: When to use MMR**\n",
    ">\n",
    "> Use **`search_type=\"mmr\"`** when you want to avoid getting multiple, very similar chunks of text in your results. It's perfect for when your source documents contain lots of overlapping or redundant information and you want to provide the LLM with a broader, more diverse context.\n",
    "\n",
    "-----\n",
    "\n",
    "### Example 3: Dense Search with Metadata Filtering\n",
    "\n",
    "This is where you can combine the best of both worlds: semantic meaning and precise filtering.\n",
    "\n",
    "```python\n",
    "# Create a retriever that ONLY searches within documents from the \"faq\" source\n",
    "faq_retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1, \"filter\": {\"source\": \"faq\"}}\n",
    ")\n",
    "\n",
    "query_filtered = \"How do I get into my account?\"\n",
    "retrieved_docs_filtered = faq_retriever.invoke(query_filtered)\n",
    "\n",
    "print(f\"\\n--- Filtered Dense Search for: '{query_filtered}' (Source='faq') ---\")\n",
    "for i, doc in enumerate(retrieved_docs_filtered):\n",
    "    print(f\"Doc {i+1}: Content='{doc.page_content}'\")\n",
    "    print(f\"   Source='{doc.metadata.get('source')}'\\n\")\n",
    "\n",
    "\n",
    "# Now, let's try a query that would NORMALLY match the FAQ, but we filter it out\n",
    "query_wont_match = \"password help\"\n",
    "no_results = faq_retriever.invoke(\"arctic animals\") # This query is semantically about animals\n",
    "print(f\"\\n--- Filtered Search for 'arctic animals' (Source='faq') ---\")\n",
    "print(f\"Number of documents found: {len(no_results)}\")\n",
    "print(\"As expected, no results are found because the animal docs don't match the 'source: faq' filter.\\n\")\n",
    "\n",
    "\n",
    "# Example with a numerical filter (year must be 2024)\n",
    "# Note: ChromaDB supports operators like $eq, $ne, $gt, $gte, $lt, $lte\n",
    "recent_report_retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2, \"filter\": {\"year\": {\"$eq\": 2024}}}\n",
    ")\n",
    "\n",
    "query_numerical = \"What are the latest developments?\"\n",
    "retrieved_num_filtered = recent_report_retriever.invoke(query_numerical)\n",
    "print(f\"\\n--- Filtered Dense Search for: '{query_numerical}' (Year = 2024) ---\")\n",
    "for i, doc in enumerate(retrieved_num_filtered):\n",
    "    print(f\"Doc {i+1}: Content='{doc.page_content}'\")\n",
    "    print(f\"   Source='{doc.metadata.get('source')}', Year='{doc.metadata.get('year')}'\\n\")\n",
    "```\n",
    "\n",
    "**Expected Output Analysis:**\n",
    "\n",
    "The first query works perfectly, finding the relevant FAQ document. The second query demonstrates the power of the filter: even though \"arctic animals\" is semantically closest to the nature document, the filter `{\"source\": \"faq\"}` prevents the retriever from even considering it, resulting in zero documents returned. The final example shows how you can apply this to numerical data, retrieving only documents from the year 2024.\n",
    "\n",
    "-----\n",
    "\n",
    "## 6\\. Choosing the Right Embedding Model (Revisited)\n",
    "\n",
    "This is a point worth reinforcing: **the quality of your dense search is fundamentally limited by the quality of your embedding model.**\n",
    "\n",
    "Remember our golden rule: **You must use the same embedding model during querying as you did during indexing.**\n",
    "\n",
    "When selecting a model, here are the factors you should weigh:\n",
    "\n",
    "  * **Performance/Accuracy:** How well does the model capture the nuances of your specific domain (e.g., finance, law, medicine)? General-purpose models are great, but specialized models might perform better on technical text. You can consult leaderboards like the MTEB (Massive Text Embedding Benchmark) to compare models.\n",
    "  * **Cost:** API-based models like OpenAI's `text-embedding-3-large` or Google's `text-embedding-004` have a cost associated with every query and every document you embed.\n",
    "  * **Latency & Privacy:** Local, open-source models from Hugging Face (like the `sentence-transformers` we used) are free to use and keep your data within your own environment. This eliminates network latency and enhances data privacy.\n",
    "  * **Model Size & Resources:** Local models require computational resources (CPU/GPU, RAM). Smaller models like `all-MiniLM-L6-v2` are fast and lightweight. Larger models like `all-mpnet-base-v2` are often more accurate but require more resources and are slightly slower.\n",
    "\n",
    "> **Pro-Tip:** Start with a well-regarded, general-purpose open-source model like `all-MiniLM-L6-v2`. It provides a fantastic balance of speed and performance and is a perfect starting point for most projects. If you find its performance lacking for your specific use case, you can then explore larger or more specialized models.\n",
    "\n",
    "-----\n",
    "\n",
    "## 7\\. Best Practices for Effective Dense Search\n",
    "\n",
    "To get the most out of your retrieval system, keep these best practices in mind:\n",
    "\n",
    "1.  **Garbage In, Garbage Out:** The quality of your embeddings is paramount. This starts with clean, well-structured source documents.\n",
    "2.  **Find the Right `k`:** Experiment to find the optimal number of documents (`k`) to retrieve for your use case. Start with `k=3` or `k=5` and evaluate the results. Is the LLM getting enough context? Or is it getting confused by too much noise?\n",
    "3.  **Chunking Matters:** Your chunking strategy directly impacts retrieval. Dense search operates on chunks, not whole documents. Ensure your chunks are self-contained and semantically meaningful. If a chunk is too small, it may lack context. If it's too large, its core meaning might be diluted.\n",
    "4.  **Leverage Metadata Filters:** This is your secret weapon for high-precision RAG. By combining semantic search with metadata filtering, you can drastically narrow down the search space and provide the LLM with highly relevant, pre-qualified information.\n",
    "5.  **Evaluate, Evaluate, Evaluate:** Don't just assume your retriever is working well. Continuously test it with a variety of queries. Look at the documents it retrieves. Are they relevant? Are they diverse? We'll cover formal evaluation methods in a future lesson, but for now, manual inspection is key.\n",
    "\n",
    "-----\n",
    "\n",
    "## 8\\. Troubleshooting Common Dense Search Issues\n",
    "\n",
    "When your retriever doesn't return what you expect, here are some common culprits and how to fix them.\n",
    "\n",
    "  * **Problem: Results are semantically irrelevant.**\n",
    "\n",
    "      * **Possible Cause:** Did you accidentally use a different embedding model for your query than for your documents? Double-check that the same model instance is used for both.\n",
    "      * **Possible Cause:** Is your embedding model a poor fit for your document's domain? A generic model might struggle with highly specialized legal or scientific jargon.\n",
    "      * **Possible Cause:** Your `k` value might be too high, pulling in documents that are only tangentially related. Try reducing `k`.\n",
    "      * **Possible Cause:** Your chunks might be too large, mixing too many different ideas into a single vector and \"muddying\" its semantic meaning. Revisit your chunking strategy.\n",
    "\n",
    "  * **Problem: The retriever misses obviously relevant documents (low recall).**\n",
    "\n",
    "      * **Possible Cause:** Your `k` value is too low. You're simply not asking for enough documents back. Try increasing `k`.\n",
    "      * **Possible Cause:** Your chunks might be too small, splitting a single coherent idea across multiple chunks. The retriever might find one piece, but the rest of the context is lost.\n",
    "      * **Possible Cause:** The user's query is highly ambiguous. The resulting query vector might not be pointing in the right \"direction\" in the vector space.\n",
    "\n",
    "  * **Problem: The search is too slow.**\n",
    "\n",
    "      * **Possible Cause:** If you're using MMR, your `fetch_k` value might be excessively high, causing the retriever to process a huge number of candidates.\n",
    "      * **Possible Cause:** For very large datasets, an in-memory vector store like FAISS might start to slow down. You might need to consider a more scalable, production-grade vector database solution.\n",
    "\n",
    "-----\n",
    "\n",
    "## 9\\. Key Takeaways\n",
    "\n",
    "> | Concept | Description |\n",
    "> | :--- | :--- |\n",
    "> | **Dense Search** | The core retrieval method for RAG, using vector embeddings to find documents based on semantic meaning, not just keywords. |\n",
    "> | **`VectorStoreRetriever`** | The primary LangChain interface for all dense search operations, created via `vectorstore.as_retriever()`. |\n",
    "> | **`search_type=\"similarity\"`** | The default search type. Finds the top `k` most semantically similar documents to a query. |\n",
    "> | **`search_type=\"mmr\"`** | Maximal Marginal Relevance. A search type that balances relevance to the query with the diversity of the results, preventing redundant outputs. |\n",
    "> | **`search_kwargs`** | A dictionary for passing parameters to the search. The most important are `k` (number of results) and `filter` (metadata filtering). |\n",
    "> | **Metadata Filtering** | A powerful technique to combine \"fuzzy\" semantic search with \"exact\" attribute matching, drastically improving precision. |\n",
    "> | **Golden Rule** | Always use the **same embedding model** for indexing your documents and for embedding your queries. |\n",
    "\n",
    "-----\n",
    "\n",
    "## 10\\. Exercises & Thought Experiments\n",
    "\n",
    "1.  **The `k` Value Impact:** Take the `retriever_similarity` from our first example. Run the same query (\"What's the latest news on artificial intelligence in the company?\") three times with `k=1`, `k=2`, and `k=3`. Print the results for each run. How does the context available for an LLM change as `k` increases?\n",
    "\n",
    "2.  **MMR vs. Similarity:** Using the documents from our MMR code example, change the query to be just \"account access problems\". Run this query with both the `retriever_similarity` and the `mmr_retriever`. Compare the results. Do you see a difference in this more focused query? Why or why not?\n",
    "\n",
    "3.  **Metadata Filtering Challenge:** Add a new piece of metadata, `\"department\": \"HR\"`, to one of the \"faq\" documents. Now, create a retriever that finds documents related to \"account access\" but *only* if the `department` is `HR`. Test it to see if it correctly isolates the single document.\n",
    "\n",
    "4.  **Model Comparison (Optional):** If you have access to the OpenAI API, replace the `HuggingFaceEmbeddings` with `OpenAIEmbeddings`. Re-run the first example (Basic Similarity Search). Do the retrieved documents or their order change at all? This can give you an intuition for how different models \"interpret\" the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec0090-c535-4b97-84fe-2e9fbee6e84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
