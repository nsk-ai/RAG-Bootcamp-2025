{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39977fc-b673-4dbb-8ac0-55ead89d013c",
   "metadata": {},
   "source": [
    "# **Level 4 - The Quest: Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6ca4c-ac5c-4812-b2ab-db6f67b7bd71",
   "metadata": {},
   "source": [
    "## Part 3: Sparse Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d4a28-4661-448f-80a2-c0bbec5fa6d7",
   "metadata": {},
   "source": [
    "Hello everyone, and welcome back to Level 4: The Quest for Retrieval\\! I'm thrilled to see you all again as we venture deeper into the heart of what makes RAG systems truly powerful.\n",
    "\n",
    "Today, we're tackling **Part 3: Sparse Search**. We'll unravel what \"sparse\" means, meet the undisputed champion of this domain—an algorithm called **BM25**—and see how it gives us a much more nuanced and powerful way to handle keyword-based retrieval.\n",
    "\n",
    "Let's get started\\!\n",
    "\n",
    "-----\n",
    "\n",
    "### 1\\. Recap & Our Journey So Far\n",
    "\n",
    "Remember our ultimate goal in retrieval: we are curators for our Large Language Model. We need to find the *most relevant, context-rich information* from our knowledge base (what we've called our \"Archives\") to help the LLM generate the best possible answer.\n",
    "\n",
    "So far, our journey has taken us through:\n",
    "\n",
    "  * **The Archives (Indexing):** We learned how to build our library by loading documents, chunking them into manageable pieces, creating vector embeddings (the dense, meaning-based representations), and storing them in vector stores like ChromaDB.\n",
    "  * **The Quest Begins (Retrieval):** In our last session, we took our first step with **Keyword Search**. It was simple and effective for finding exact word matches. If you searched for \"Project Phoenix,\" it found documents containing \"Project Phoenix.\"\n",
    "\n",
    "But we also hit a wall, a fundamental limitation known as the **Lexical Gap**. This is the gap between the words a user types and the words used in the document to describe the same concept. Simple keyword search can't cross this gap. It's literal, and it lacks \"common sense.\"\n",
    "\n",
    "Today, we're building a bridge. We're introducing **Sparse Search** with BM25, a method that's still fundamentally about keywords but is far more intelligent. It doesn't just ask \"Is the word here?\" It asks, \"**How important is this word to this document?**\"\n",
    "\n",
    "### 2\\. The Problem: The Flaws of Simple Keyword Search\n",
    "\n",
    "Let's revisit why we need something better than a simple `Ctrl+F`. Simple keyword search is brittle.\n",
    "\n",
    "  * **The \"Car\" vs. \"Automobile\" Problem:** If your query is \"technical specs for the new car,\" and your document says \"technical specs for the new automobile,\" a simple keyword search will fail completely. The lexical gap strikes again.\n",
    "  * **Ignoring Word Importance:** It treats every word equally. In the query \"report on AI safety protocols,\" the words \"AI\" and \"safety\" are far more significant than \"report\" or \"on.\" A simple search doesn't know this.\n",
    "  * **The Common Word Plague:** Words like \"the,\" \"is,\" \"a,\" and \"in\" (stop words) appear everywhere. They add noise and have almost zero value in finding relevant content, yet a naive search might give them weight.\n",
    "  * **No Concept of Rarity:** Imagine you search for \"sparse search with BM25.\" The term \"BM25\" is very specific and likely a strong indicator of relevance. The word \"with\" is not. Simple keyword search has no mechanism to understand that rare terms are often more powerful signals.\n",
    "\n",
    "We need a system that can weigh these factors, and that's precisely what sparse search models are designed to do.\n",
    "\n",
    "### 3\\. What is Sparse Search? The \"Sparse\" Vector Explained\n",
    "\n",
    "Okay, let's tackle the name: \"Sparse Search.\" It comes from the data structure it uses: **sparse vectors**.\n",
    "\n",
    "Remember the dense vectors from our embedding models (like `text-embedding-ada-002`)? They were relatively short (e.g., 1536 dimensions) but \"dense,\" meaning almost every one of those 1536 numbers had a non-zero value, collectively capturing the *meaning* of the text.\n",
    "\n",
    "Sparse vectors are the complete opposite.\n",
    "\n",
    "  * **Core Concept:** A sparse vector has a dimension for *every single unique word* in your entire vocabulary. If your document collection has 50,000 unique words, the sparse vector for each document will have 50,000 dimensions\\!\n",
    "  * **Mostly Zeros:** For any given document, most of these dimensions will be **zero**. Only the dimensions corresponding to words that are *actually present* in that document will have a non-zero value. This value represents the \"importance\" or \"weight\" of that word in that document.\n",
    "\n",
    "**Analogy: The Giant Vocabulary Checklist**\n",
    "\n",
    "Imagine you have a giant checklist representing every word in the English language (our vocabulary).\n",
    "\n",
    "  * To create a **sparse vector** for a single document (like one of our chunks), you go down this massive checklist.\n",
    "  * You put a **zero** next to every word that *is not* in your document.\n",
    "  * For the words that *are* in your document, you write down a number—a score—that represents how important that word is.\n",
    "\n",
    "The result? A very long checklist where most entries are '0'. It is \"sparse.\"\n",
    "\n",
    "This is fundamentally different from a dense vector, which uses a complex, compressed representation of meaning. Sparse vectors are an explicit, one-to-one map of words to dimensions.\n",
    "\n",
    "### 4\\. Introducing BM25: The Champion of Sparse Search\n",
    "\n",
    "The most famous, battle-tested, and effective algorithm for calculating the \"importance\" scores in these sparse vectors is **BM25**, which stands for **Best Matching 25**. (It was the 25th iteration of a series of formulas, and it stuck\\!)\n",
    "\n",
    "Developed in the 1980s and 90s, BM25 is the workhorse behind countless search engines, including giants like Elasticsearch and Lucene. It's a classic for a reason: it works exceptionally well.\n",
    "\n",
    "So, how does this \"smart librarian\" decide how important a word is? It masterfully balances three key ideas:\n",
    "\n",
    "1.  **Term Frequency (TF):** How often does a query word appear in a specific document? A document that mentions \"solar panels\" five times is likely more relevant to the query \"solar panels\" than a document that mentions it only once.\n",
    "2.  **Inverse Document Frequency (IDF):** How rare is that word across *all* documents in your collection? The term \"photosynthesis\" is rare and specific, so it's a powerful signal. The term \"science\" is more common. The term \"the\" is everywhere and useless. IDF gives a massive boost to rare, distinctive terms.\n",
    "3.  **Document Length Normalization:** Should a 500-page book that mentions your search term 20 times always rank higher than a 2-page, highly focused article that mentions it 10 times? Not necessarily. BM25 adjusts the score to account for document length, so longer documents don't have an unfair advantage just because they have more words.\n",
    "\n",
    "It also includes a fourth, subtle concept:\n",
    "\n",
    "  * **Term Frequency Saturation:** The benefit of a word appearing over and over again diminishes. The difference in relevance between 1 and 5 occurrences is huge. The difference between 101 and 105 occurrences is tiny. BM25 has a parameter (`k1`) that controls how quickly this saturation happens, preventing \"keyword stuffing\" from dominating the results.\n",
    "\n",
    "### 5\\. Strengths of Sparse Search (BM25)\n",
    "\n",
    "Why do we still use an algorithm from the 90s in our state-of-the-art RAG systems? Because it's incredibly good at what it does.\n",
    "\n",
    "  * **Unmatched for Keyword Relevance:** When your query involves specific names, product codes, error messages, legal terms, or jargon, BM25 is your best friend. It excels at finding documents with those exact terms.\n",
    "  * **Interpretability:** This is a huge advantage\\! When BM25 retrieves a document, you know *exactly why*. You can see which keywords in your query matched and what their scores were. This is much harder with the abstract, black-box nature of dense vectors.\n",
    "  * **Computationally Efficient:** Creating a BM25 index is often faster than generating embeddings for millions of documents. Queries are also extremely fast, making it ideal for a first-pass retrieval step.\n",
    "  * **No Training Required:** Unlike some complex neural models, BM25 works out-of-the-box by analyzing the statistical properties of your text corpus.\n",
    "\n",
    "### 6\\. Weaknesses of Sparse Search (BM25)\n",
    "\n",
    "Of course, BM25 isn't perfect. Its strengths are also the source of its weaknesses.\n",
    "\n",
    "  * **The Lexical Gap Still Exists:** It has **no true semantic understanding**. If you search for \"feeling happy,\" it will not match a document that says \"a state of pure joy.\" The words are different, so to BM25, the documents are unrelated. This is where dense, semantic search shines.\n",
    "  * **Out-of-Vocabulary (OOV) Words:** If your query contains a word that was not in the original set of documents used to build the vocabulary, BM25 cannot search for it. The word simply doesn't have a dimension in the sparse vector.\n",
    "  * **Context and Relationships are Ignored:** BM25 sees a document as a \"bag of words.\" It doesn't understand that \"man bites dog\" is very different from \"dog bites man.\" It only knows that the words \"man,\" \"bites,\" and \"dog\" are present.\n",
    "\n",
    "### 7\\. When Should You Use Sparse Search in a RAG System?\n",
    "\n",
    "Understanding these trade-offs tells us exactly where to deploy BM25:\n",
    "\n",
    "1.  **For Queries with Specific Identifiers:** Searching for `error code 404`, `product SKU X-7B3`, `case number 98-C-1138`, or `Dr. Evelyn Reed`. Dense search might find *conceptually similar* things, but BM25 will find the *exact match*.\n",
    "2.  **In Domains with Critical Terminology:** In legal, medical, or scientific research, the exact wording (`pro-bono` vs. `free`, `myocardial infarction` vs. `heart attack`) is non-negotiable. BM25 respects this precision.\n",
    "3.  **As the Foundation for Hybrid Search:** This is the key takeaway\\! The ultimate goal isn't to choose between sparse and dense search. The goal is to **combine them**. We'll cover this soon, but the idea is to get the \"best of both worlds\": the keyword precision of BM25 and the semantic understanding of dense embeddings.\n",
    "4.  **As a Fast Initial Filter:** For massive document collections, you can use a quick BM25 search to retrieve the top 100 candidates and then use a more computationally expensive dense search or reranker on just that small subset.\n",
    "\n",
    "### 8\\. Implementing Sparse Search in LangChain: The `BM25Retriever`\n",
    "\n",
    "Enough theory\\! Let's see how easy LangChain makes this. The key object is the `BM25Retriever`.\n",
    "\n",
    "First, you'll need to install its dependency:\n",
    "\n",
    "```bash\n",
    "pip install rank_bm25\n",
    "```\n",
    "\n",
    "Now, let's dive into the code. The `BM25Retriever` is a `Runnable`, just like our other components, so it fits perfectly into our chains.\n",
    "\n",
    "#### Example 1: Basic BM25 Retrieval\n",
    "\n",
    "Here, we'll initialize the retriever from a list of `Document` chunks and see how it handles different types of queries.\n",
    "\n",
    "```python\n",
    "# --- Setup: Imagine these are your chunks ---\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "sample_chunks = [\n",
    "    Document(page_content=\"The cat sat on the mat. It was a fluffy cat.\", metadata={\"source\": \"story-1\", \"page\": 1}),\n",
    "    Document(page_content=\"The dog chased the ball. The ball was red.\", metadata={\"source\": \"story-2\", \"page\": 1}),\n",
    "    Document(page_content=\"A fluffy feline rested on the soft rug.\", metadata={\"source\": \"story-1\", \"page\": 2}),\n",
    "    Document(page_content=\"Dogs are known for their loyalty and playful nature.\", metadata={\"source\": \"story-2\", \"page\": 2}),\n",
    "    Document(page_content=\"The new project report for Q3 is due next week.\", metadata={\"source\": \"report-q3\"}),\n",
    "    Document(page_content=\"Q4 financial results show significant growth.\", metadata={\"source\": \"report-q4\"}),\n",
    "]\n",
    "\n",
    "# --- BM25 Implementation ---\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# 1. Initialize BM25Retriever from our documents\n",
    "bm25_retriever = BM25Retriever.from_documents(sample_chunks)\n",
    "bm25_retriever.k = 2 # We want the top 2 results\n",
    "\n",
    "# --- Let's test it! ---\n",
    "print(\"--- BM25 Retriever Demo ---\")\n",
    "\n",
    "# Test 1: A query with strong keyword overlap\n",
    "query1 = \"fluffy cat\"\n",
    "print(f\"\\nQuery: '{query1}'\")\n",
    "retrieved_docs_1 = bm25_retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs_1):\n",
    "    print(f\"Doc {i+1} (Source: {doc.metadata.get('source')}): {doc.page_content}\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Test 2: A query with a specific, rare term\n",
    "query2 = \"Q3 report\"\n",
    "print(f\"\\nQuery: '{query2}'\")\n",
    "retrieved_docs_2 = bm25_retriever.invoke(query2)\n",
    "for i, doc in enumerate(retrieved_docs_2):\n",
    "    print(f\"Doc {i+1} (Source: {doc.metadata.get('source')}): {doc.page_content}\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Test 3: A semantic query where keywords DON'T match (demonstrating the weakness)\n",
    "query3 = \"What kind of pet sits on the floor covering?\"\n",
    "print(f\"\\nQuery: '{query3}'\")\n",
    "retrieved_docs_3 = bm25_retriever.invoke(query3)\n",
    "for i, doc in enumerate(retrieved_docs_3):\n",
    "    print(f\"Doc {i+1} (Source: {doc.metadata.get('source')}): {doc.page_content}\")\n",
    "\n",
    "```\n",
    "\n",
    "**Discussion of Output:**\n",
    "\n",
    "  * For `\"fluffy cat\"`, the first document is a perfect match because it contains both \"fluffy\" and \"cat\". The third document (\"fluffy feline rested on the soft rug\") is also retrieved because \"fluffy\" is a strong keyword match, but it will have a lower score than the first document.\n",
    "  * For `\"Q3 report\"`, BM25 will almost certainly retrieve the document containing `\"project report for Q3\"` as the top result. The terms \"Q3\" and \"report\" are rare in our small dataset, giving them a high IDF score.\n",
    "  * For `\"What kind of pet sits on the floor covering?\"`, the results will be poor. BM25 has no idea that \"feline\" is a \"pet\" or that a \"mat\" or \"rug\" is a \"floor covering.\" It will likely match on common words like \"on,\" \"the,\" or \"of,\" retrieving irrelevant documents. **This perfectly illustrates the lexical gap and why we also need dense retrieval.**\n",
    "\n",
    "#### Example 2: Using a `preprocess_func`\n",
    "\n",
    "Sometimes, you want to normalize your text before BM25 analyzes it. A common step is to lowercase everything so that \"Cat\" and \"cat\" are treated as the same word. The `preprocess_func` parameter is perfect for this.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# You may need to download this once:\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define a simple preprocessing function\n",
    "def custom_preprocess(text: str) -> list[str]:\n",
    "    \"\"\"Converts text to lowercase and splits it into words.\"\"\"\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Initialize a new retriever with our function\n",
    "bm25_retriever_processed = BM25Retriever.from_documents(\n",
    "    sample_chunks,\n",
    "    preprocess_func=custom_preprocess\n",
    ")\n",
    "bm25_retriever_processed.k = 2\n",
    "\n",
    "query_processed = \"FLUFFY feliNE\" # Test with messy casing\n",
    "print(f\"\\n--- BM25 Retriever with Preprocessing Demo ---\")\n",
    "print(f\"\\nQuery: '{query_processed}' (using preprocessed BM25)\")\n",
    "retrieved_docs_processed = bm25_retriever_processed.invoke(query_processed)\n",
    "for i, doc in enumerate(retrieved_docs_processed):\n",
    "    print(f\"Doc {i+1} (Source: {doc.metadata.get('source')}): {doc.page_content}\")\n",
    "```\n",
    "\n",
    "Here, by preprocessing both the documents and the query to be lowercase, we make our search robust to capitalization differences, retrieving the \"fluffy feline\" document correctly.\n",
    "\n",
    "### 9\\. Comparing Sparse vs. Dense Search (Foreshadowing Hybrid)\n",
    "\n",
    "Let's put it on a scorecard.\n",
    "\n",
    "| Feature               | **Sparse Search (BM25)** | **Dense Search (Embeddings)** |\n",
    "| --------------------- | ---------------------------------------------------------- | -------------------------------------------------------------- |\n",
    "| **Matching Type** | **Lexical** (Keywords)                                     | **Semantic** (Meaning, Concepts)                               |\n",
    "| **Understands Synonyms?** | **No.** `\"car\"` does not match `\"automobile\"`.                 | **Yes.** `\"car\"` is semantically close to `\"automobile\"`.      |\n",
    "| **Handles Jargon/Codes?** | **Excellent.** Perfect for specific, exact terms.          | **Poor.** Can get confused by terms not in its training data. |\n",
    "| **Interpretability** | **High.** You know exactly which keywords caused a match.    | **Low.** The vector space is a \"black box.\"                     |\n",
    "| **Speed** | Generally **very fast** for indexing and retrieval.        | Can be slower/more resource-intensive, especially for indexing. |\n",
    "| **\"Best For\"** | Queries requiring precision and exact term matching.       | Queries requiring conceptual understanding and paraphrasing.     |\n",
    "\n",
    "As you can see, there is no single winner. They are two different tools for two different jobs. And as any good craftsperson knows, the best results often come from using multiple tools together. This is the entire premise of **Hybrid Search**, which we'll explore soon.\n",
    "\n",
    "### 10\\. Troubleshooting & Practical Tips\n",
    "\n",
    "  * **Garbage In, Garbage Out:** The quality of your text matters. Misspellings, typos, and inconsistent formatting in your source documents will degrade BM25's performance. Cleaning your data is always time well spent.\n",
    "  * **The `k` Parameter:** Your main tuning knob is `k`, the number of documents to retrieve. Start with a reasonable number (4-5) and adjust based on performance.\n",
    "  * **Metadata Filtering:** The base `BM25Retriever` does not have a built-in mechanism for filtering by metadata *during* the search (like `VectorStoreRetriever` does). If you need to search only within documents where `source=\"report-q3\"`, you should pre-filter your list of `Documents` *before* passing them to `BM25Retriever.from_documents()`.\n",
    "  * **Performance:** BM25 is CPU-bound and generally very fast. You won't typically run into performance bottlenecks with it unless your document corpus is truly astronomical in size.\n",
    "\n",
    "### 11\\. Key Takeaways\n",
    "\n",
    "> | **Sparse Search & BM25: The Essentials** |\n",
    "> | ---------------------------------------------------------------------------------------------------- |\n",
    "> | **What it is:** A keyword-based retrieval algorithm that scores documents based on term importance.      |\n",
    "> | **Core Logic:** Balances **Term Frequency (TF)**, **Inverse Document Frequency (IDF)**, and document length. |\n",
    "> | **Vector Type:** Uses **Sparse Vectors** (high-dimensional, mostly zeros).                           |\n",
    "> | **Strengths:** Excellent for exact matches, specific terms, jargon, and interpretability.            |\n",
    "> | **Weaknesses:** Fails to understand synonyms or true conceptual meaning (the lexical gap).              |\n",
    "> | **LangChain Class:** Implemented via `langchain_community.retrievers.BM25Retriever`.                 |\n",
    "> | **The Big Picture:** A critical component for building robust **Hybrid Search** systems.               |\n",
    "\n",
    "### 12\\. Exercises & Thought Experiments\n",
    "\n",
    "Time to get your hands dirty and build some intuition\\!\n",
    "\n",
    "1.  **The Synonym Test:** Create a small list of `Document` objects containing clear synonyms. For example:\n",
    "\n",
    "      * `Document(page_content=\"The new CEO is very intelligent.\")`\n",
    "      * `Document(page_content=\"Our company hired a brilliant manager.\")`\n",
    "      * `Document(page_content=\"She is a smart and effective leader.\")`\n",
    "      * Initialize a `BM25Retriever` with these documents. Query for `\"smart CEO\"`. What do you get back? Why? Now, query for `\"intelligent leader\"`. What does this tell you about how BM25 \"thinks\"?\n",
    "\n",
    "2.  **The Jargon Test:** Find a technical paragraph from a field you know well (coding, engineering, finance, etc.). Create a document from it. Now, form two queries:\n",
    "\n",
    "      * A query using the *exact* jargon from the text.\n",
    "      * A query explaining the *concept* of the jargon in simple terms.\n",
    "      * Run both through `BM25Retriever`. Compare the results. This will highlight where BM25 shines and where it fails.\n",
    "\n",
    "3.  **The Length Bias Experiment:** Create two documents:\n",
    "\n",
    "      * Doc 1 (short): `\"The key finding is the isolation of the LZ-9 enzyme.\"`\n",
    "      * Doc 2 (long): A long paragraph about lab procedures that mentions the \"LZ-9 enzyme\" only once in the middle of many other sentences.\n",
    "      * Query for `\"LZ-9 enzyme\"`. Which document does BM25 rank higher? Its length normalization should favor the shorter, more focused document.\n",
    "\n",
    "You've made incredible progress today. You've moved beyond simple search and have now mastered a sophisticated, statistically-backed method for lexical retrieval. This is a vital skill and a cornerstone of production-grade RAG systems.\n",
    "\n",
    "In our next session, we'll got indepth on Dense Vectors, then finally unite the two worlds: the keyword precision of BM25 and the semantic power of dense embeddings. Get ready for Hybrid Search\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecabaac-7cdd-4c8a-bf0c-d2481325b246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
